{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;font-size:30px;font-weight:bold\">Clause Syntax in the Song of Songs:<br><br> A Preliminary Study</p>\n",
    "<br>\n",
    "<br>\n",
    "# Song of Songs Translation Processing\n",
    "<strong>Purpose of this notebook:</strong>\n",
    "<br>\n",
    "<br>\n",
    "Since English translations need to be manually compared with the Hebrew text, this code creates files that facilitate faster data input for the user. The code uses NLTK (Natural Language Tool Kit), a Python module, to analyze the English translations and print a clean format to a text file. The code takes in the plaintext translation (ex: NAS > plaintext. An example of the result can be viewed in the translation folder (NAS, for example, under the format NAS_ch1_coded.txt).  \n",
    "<br>\n",
    "<br>\n",
    "The text below provides a brief example of the result from the NAS chapter 1:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "--------------------------------------------------\n",
    "2\n",
    "\"may he kiss me with the kisses of his mouth\n",
    "     for your love is better than wine.\n",
    "\n",
    "may        kiss       is         \n",
    "mod        pres       pres       \n",
    "\n",
    "יִשָּׁקֵ֨נִי֙        \n",
    "\n",
    "ZYq0       AjCl       \n",
    "\n",
    "\n",
    "--------------------------------------------------\n",
    "<br>\n",
    "<br>\n",
    "The notebook shows the result for the NAS. To process each translation, \"filename\" in cell 5 is modified to reflect the three-letter translation code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.5.21\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: https://shebanq.ancient-data.org/static/docs/featuredoc/texts/welcome.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import collections\n",
    "from laf.fabric import LafFabric\n",
    "from etcbc.preprocess import prepare\n",
    "from etcbc.lib import Transcription\n",
    "fabric = LafFabric(verbose='NORMAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s USING main  DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.00s USING annox DATA COMPILED AT: 2016-01-27T19-01-17\n",
      "  4.53s LOGFILE=/Users/Cody/laf-fabric-output/etcbc4b/monad_boundaries_input/__log__monad_boundaries_input.txt\n",
      "  4.53s INFO: LOADING PREPARED data: please wait ... \n",
      "  4.53s prep prep: G.node_sort\n",
      "  4.66s prep prep: G.node_sort_inv\n",
      "  5.45s prep prep: L.node_up\n",
      "  9.42s prep prep: L.node_down\n",
      "    17s prep prep: V.verses\n",
      "    17s prep prep: V.books_la\n",
      "    17s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "    20s INFO: LOADED PREPARED data\n",
      "    20s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK monad_boundaries_input AT 2016-05-11T16-37-50\n"
     ]
    }
   ],
   "source": [
    "API = fabric.load('etcbc4b', 'lexicon', 'monad_boundaries_input',\n",
    "{\n",
    "    \"xmlids\" : {\"node\": False, \"edge\" : False},\n",
    "    \"features\" : (\"otype monads g_word_utf8 typ sp label chapter verse book\",\"\"),\n",
    "    \"prepare\" : prepare\n",
    "}\n",
    "           )\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    13s Complete! 6019 appended\n"
     ]
    }
   ],
   "source": [
    "#all nodes in the Song of Songs are logged for faster searching and retrieval (nodes).\n",
    "\n",
    "nodes = []\n",
    "for n in NN():\n",
    "    if F.otype.v(n) == 'book':\n",
    "        cur_book = F.book.v(n)\n",
    "    if cur_book == 'Canticum' and F.otype.v(n) != 'book':\n",
    "        nodes.append(n)\n",
    "        \n",
    "msg('Complete! {} appended'.format(len(nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 5m 56s NAS_ch1 written\n",
      " 5m 56s NAS_ch2 written\n",
      " 5m 56s NAS_ch3 written\n",
      " 5m 57s NAS_ch4 written\n",
      " 5m 57s NAS_ch5 written\n",
      " 5m 57s NAS_ch6 written\n",
      " 5m 57s NAS_ch7 written\n",
      " 5m 57s NAS_ch8 written\n"
     ]
    }
   ],
   "source": [
    "#The loop moves through all 8 chapters of the Song by opening the necessary file and producing a modified version. \n",
    "#There are some necessary adjustments for the versification differences \n",
    "#between the Hebrew and English texts. This problem created an interesting dillemma.\n",
    "#As a work-around, chapters 6 and 7 are dealth with differently.\n",
    "\n",
    "ct = 1\n",
    "\n",
    "while ct <= 8:\n",
    "    tar_chapter = '{}'.format(str(ct))\n",
    "    filename = 'NAS_ch{}'.format(str(ct))\n",
    "\n",
    "    cur_book = None\n",
    "\n",
    "    eng_ch1 = []\n",
    "\n",
    "    with open('{}.txt'.format(filename), 'r') as txtfile:\n",
    "        for line in txtfile:\n",
    "            eng_ch1.append(line.strip().lower())\n",
    "        \n",
    "    new_ch = []\n",
    "    sen_vs = collections.OrderedDict([])\n",
    "\n",
    "    for verse in eng_ch1:\n",
    "        new_ch.append(re.split(';\\s|\\?\\s|\\!\\s|\\.\\s',verse))\n",
    "\n",
    "    for verse in new_ch:\n",
    "        if verse[0][1] != ' ':\n",
    "            vrs = verse[0][0]+verse[0][1]\n",
    "        else:\n",
    "            vrs = verse[0][0]\n",
    "        sen_vs[vrs] = []\n",
    "        for sentence in verse:\n",
    "            if sentence == verse[0]:\n",
    "                sen_vs[vrs].append(sentence[2:])\n",
    "            else: \n",
    "                sen_vs[vrs].append(sentence)\n",
    "            \n",
    "\n",
    "    POS_index = {'VB' : 'pres','VBD': 'past','VBG':'PSTptc','VBN': 'PTptc','VBP': 'pres',\n",
    "                'VBZ': 'pres','MD':'mod'}\n",
    "    eng_verbs = collections.OrderedDict([])\n",
    "\n",
    "    for verse in sen_vs:\n",
    "    \n",
    "        for sentence in sen_vs[verse]:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            tagged = nltk.pos_tag(tokens)\n",
    "            for item in tagged:\n",
    "                if re.match('VB?|MD',item[1]):\n",
    "                    if verse not in eng_verbs:\n",
    "                        eng_verbs[verse] = []\n",
    "                        eng_verbs[verse].append([item[0],POS_index[item[1]]])\n",
    "                    else:\n",
    "                        eng_verbs[verse].append([item[0],POS_index[item[1]]])\n",
    "    \n",
    "        if verse not in eng_verbs:\n",
    "            eng_verbs[verse] = ''\n",
    "        \n",
    "    cur_chapter = None\n",
    "    cur_verse = None\n",
    "\n",
    "    cl_types = collections.OrderedDict([])\n",
    "\n",
    "    #adjustment for chapter 7 versification difference\n",
    "    adjustment = 1 if tar_chapter == '7' else 0\n",
    "\n",
    "    for n in nodes:\n",
    "        otype = F.otype.v(n)\n",
    "        if otype == 'chapter':\n",
    "            cur_chapter = F.chapter.v(n)\n",
    "        if otype == 'clause' and cur_chapter == tar_chapter:\n",
    "            if str(int(F.verse.v(L.u('verse',n)))-adjustment) not in cl_types:\n",
    "                cl_types[str(int(F.verse.v(L.u('verse',n)))-adjustment)] = [F.typ.v(n)]\n",
    "            elif str(int(F.verse.v(L.u('verse',n)))-adjustment) in cl_types:\n",
    "                cl_types[str(int(F.verse.v(L.u('verse',n)))-adjustment)].append(F.typ.v(n))\n",
    "\n",
    "    #to adjust for eng / heb versification difference\n",
    "    if tar_chapter == '6':\n",
    "        for n in nodes:\n",
    "            otype = F.otype.v(n)\n",
    "        \n",
    "            if otype == 'chapter':\n",
    "                cur_chapter = F.chapter.v(n)\n",
    "            \n",
    "            if otype == 'clause' and cur_chapter == '7' and F.verse.v(L.u('verse',n)) == '1':\n",
    "                if '13' not in cl_types:\n",
    "                    cl_types['13'] = [F.typ.v(n)]\n",
    "                elif '13' in cl_types:\n",
    "                    cl_types['13'].append(F.typ.v(n))\n",
    "                \n",
    "                \n",
    "    verbs = collections.OrderedDict([])\n",
    "\n",
    "    for n in nodes:\n",
    "        otype = F.otype.v(n)\n",
    "        if otype == 'chapter':\n",
    "            cur_chapter = F.chapter.v(n)\n",
    "        if (otype == 'word') and (cur_chapter == tar_chapter) and (F.sp.v(n)=='verb'):\n",
    "            if str(int(F.verse.v(L.u('verse',n)))-adjustment) not in verbs:\n",
    "                verbs[str(int(F.verse.v(L.u('verse',n)))-adjustment)] = [ F.g_word_utf8.v(n) ]\n",
    "            elif str(int(F.verse.v(L.u('verse',n)))-adjustment) in verbs:\n",
    "                verbs[str(int(F.verse.v(L.u('verse',n)))-adjustment)].append(F.g_word_utf8.v(n))\n",
    "            \n",
    "\n",
    "    #to adjust for eng / heb versification difference\n",
    "    if tar_chapter == '6':\n",
    "        for n in nodes:\n",
    "            otype = F.otype.v(n)\n",
    "        \n",
    "            if otype == 'chapter':\n",
    "                cur_chapter = F.chapter.v(n)\n",
    "            \n",
    "            if otype == 'word' and cur_chapter == '7' and F.sp.v(n) == 'verb' and F.verse.v(L.u('verse',n)) == '1':\n",
    "                if '13' not in verbs:\n",
    "                    verbs['13'] = [F.g_word_utf8.v(n)]\n",
    "                elif '13' in verbs:\n",
    "                    verbs['13'].append(F.g_word_utf8.v(n))\n",
    "                \n",
    "    with open('{}_coded.txt'.format(filename), 'w') as txtfile:\n",
    "        for verse in sen_vs:\n",
    "            txtfile.write(verse+'\\n')\n",
    "            indent = 0\n",
    "            for sentence in sen_vs[verse]:\n",
    "                txtfile.write((' '*indent)+sentence + '\\n')\n",
    "                indent += 5\n",
    "            txtfile.write('\\n')\n",
    "            string1 = ''\n",
    "            string2 = ''\n",
    "            string3 = ''\n",
    "            string4 = ''\n",
    "        \n",
    "            for sentence in eng_verbs[verse]:\n",
    "                string1 += '{:11}'.format(sentence[0])\n",
    "                string3 += '{:11}'.format(sentence[1])\n",
    "    \n",
    "            if verse in verbs:\n",
    "                new_order = []\n",
    "                for word in verbs[verse]:\n",
    "                    new_order.append(word)\n",
    "                new_order.reverse()\n",
    "                for word in new_order:\n",
    "                    string2 += '{:20}'.format(word)\n",
    "                \n",
    "            for typ in cl_types[verse]:\n",
    "                string4 += '{:11}'.format(typ)\n",
    "        \n",
    "            txtfile.write(string1+'\\n')\n",
    "            txtfile.write(string3+'\\n\\n')\n",
    "            txtfile.write(string2+'\\n\\n')\n",
    "            txtfile.write(string4+'\\n\\n')\n",
    "            txtfile.write('\\n')\n",
    "        \n",
    "        \n",
    "            txtfile.write('-'*50+'\\n\\n')\n",
    "        ct += 1\n",
    "        msg('{} written'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
